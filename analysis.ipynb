{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation and Cleansing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Pipeline\n",
    "\n",
    "The data preprocessing pipeline consists of several key stages to prepare the dataset for modeling:\n",
    "\n",
    "1. **Data Loading**: The `load_data` function in `data_loading.py` loads multiple CSV files and merges them into a single DataFrame using the 'id' column as the merging key.\n",
    "\n",
    "2. **Missing Value Imputation**: The `infer_missing_values_in_dataframe` function in `llm_dataset_filler.py` uses a local LLM to infer missing values for specific fields (e.g., Age, Gender, Education Level) based on the provided descriptions. It utilizes asynchronous calls to the LLM API and updates the DataFrame with inferred values.\n",
    "\n",
    "3. **Feature Engineering**: The `feature_engineering.py` file contains functions that split the data into training and testing sets (`split_data`), preprocess the training data by encoding categorical variables, target encoding 'Job Title', normalizing numerical variables (`normalize_train_data`), and preprocess the test data using the fitted encoders and scaler (`normalize_test_data`). in this step we also drop unnecesary features such as gender and id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necesary imports\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "fm._log.setLevel(logging.WARNING)\n",
    "from src import data_loading\n",
    "from src import preprocessing\n",
    "from src import visualize_data\n",
    "from src import feature_engenieering\n",
    "from src import modeling\n",
    "from src import evaluation\n",
    "from src import model_compare\n",
    "from src import inference\n",
    "from src.inference_jupyter_form import create_input_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#files path for the raw dataset:\n",
    "\n",
    "data_files = ['./data/people.csv','./data/descriptions.csv','./data/salary.csv',]\n",
    "\n",
    "\n",
    "#merge datasets in a cohesive Dataframe\n",
    "\n",
    "full_dataset = data_loading.load_data(data_files)\n",
    "\n",
    "\n",
    "#preprocessing of the dataframe adds missing values with LLM inference over descriptions of each row, drops the incomplete rows and cleans up the data.\n",
    "\n",
    "cleansed_dataset = await preprocessing.preprocess(full_dataset)\n",
    "\n",
    "visualize_data.visualize_dataset(cleansed_dataset)\n",
    "\n",
    "\n",
    "#split the dataset into an 80 / 20 ratio for training and testing.\n",
    "\n",
    "X_train, X_test, y_train, y_test = feature_engenieering.split_data(cleansed_dataset)\n",
    "\n",
    "\n",
    "#normalize and scale the datasets using MinMaxScaler and target encoder for random forest\n",
    "\n",
    "normalized_X_train, te, scaler = feature_engenieering.normalize_train_data(X_train, y_train,MinMaxScaler())\n",
    "\n",
    "normalized_X_test = feature_engenieering.normalize_test_data(X_test, te, scaler)\n",
    "\n",
    "\n",
    "#normalize and scale the datasets using MinMaxScaler and target encoder for Neural Networks\n",
    "\n",
    "normalized_X_train_nn, te_nn, scaler_nn = feature_engenieering.normalize_train_data(X_train, y_train,StandardScaler(),\"nn_\")\n",
    "\n",
    "normalized_X_test_nn = feature_engenieering.normalize_test_data(X_test, te_nn, scaler_nn)\n",
    "\n",
    "X_train.to_pickle(\"./data/X_train.pkl\")\n",
    "y_train.to_pickle(\"./data/y_train.pkl\")\n",
    "X_test.to_pickle(\"./data/X_test.pkl\")\n",
    "y_test.to_pickle(\"./data/y_test.pkl\")\n",
    "normalized_X_train.to_pickle(\"./data/normalized_X_train.pkl\")\n",
    "normalized_X_train_nn.to_pickle(\"./data/normalized_X_train_nn.pkl\")\n",
    "normalized_X_test.to_pickle(\"./data/normalized_X_test.pkl\")\n",
    "normalized_X_test_nn.to_pickle(\"./data/normalized_X_test_nn.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load generated datasets as pkl\n",
    "\n",
    "normalized_X_train = pd.read_pickle('./data/normalized_X_train.pkl')\n",
    "normalized_X_train_nn = pd.read_pickle('./data/normalized_X_train_nn.pkl')\n",
    "\n",
    "normalized_X_test = pd.read_pickle('./data/normalized_X_test.pkl')\n",
    "normalized_X_test_nn = pd.read_pickle('./data/normalized_X_test_nn.pkl')\n",
    "\n",
    "y_train = pd.read_pickle('./data/y_train.pkl')\n",
    "y_test = pd.read_pickle('./data/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Regressor Baseline model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the datasets created we use the train with MinMaxScaling dataset splits in a script to create a Random Forest Regressor using the scikit-learn framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We traing a Dummy Reggressor to use as a baseline for model performance comparison and then train a Random Forest Regressor algorithm with hyperparameter tuning. We also evaluate the trained model by calculating metrics such as mean absolute error (MAE), root mean squared error (RMSE) and R-squared e (R2) and plot a scatterplot of predicted vs actual salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy = modeling.train_dummy_regressor(normalized_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a model using a random forest regressor algorithm and print out the predictions for the normalized test data.\n",
    "\n",
    "rf_model = modeling.train_model(normalized_X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the test dataset to predict salaries based on the trained model for a first fast evaluation.\n",
    "\n",
    "\n",
    "rf_model = joblib.load(open('./models/random_forest_model.pkl', 'rb'))\n",
    "\n",
    "evaluation.evaluate_model(normalized_X_test, y_test, normalized_X_train,y_train, rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use Confidence intervals to further test the  models performance using bootstraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_model = joblib.load(open('./models/random_forest_model.pkl', 'rb'))\n",
    "\n",
    "evaluation.calculate_metrics(normalized_X_test, y_test, rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose the Random Forest Reggresor to be able to catch non linear relationships better with realtively low data quantity for training, the model is Hyperparametrized using a grid approach and the best model is selected for testing.\n",
    "\n",
    "In the process of selecting features for the Random Forest Regressor model, based on correlation and feature importance analysis after the first \"naive\" model was trained in the normalized data as is to gather more information on the relationships and model error R-squared Score (R²).\n",
    "\n",
    "it is also quite notable that the heteroscedasticity present in the middle of the distribution seems to be caused by the dataset itself, most possibly by the lack of examples to fill the distribution appropiately, otherwise the distribution of residuals seems to be acceptable considering there are no statiscally significant outliers in the dataset.\n",
    "\n",
    "        Random Forest Regressor Performance After Hyperparameter Tuning:\n",
    "        Mean Squared Error (MSE): 866064920.96\n",
    "        Mean Absolute Error (MAE): 17982.33\n",
    "        R-squared Score (R²): 0.65\n",
    "        \n",
    "![img](./plots/residuals_distribution.png)\n",
    "![Feature Importance](./plots/feature_importance.png)\n",
    "![feature correlation heatmap](./plots/feature_correlation_heatmap.png)\n",
    "\n",
    "\n",
    "\n",
    "It can be seen now better results by droping geneder since it has a really low correlation with salary and importance , and lower the smoothing for the encoded job titles to 3 (10 was the previous value).\n",
    "\n",
    "        Random Forest Regressor Performance After Hyperparameter Tuning:\n",
    "        Mean Squared Error (MSE): 600148032.12\n",
    "        Mean Absolute Error (MAE): 14748.68\n",
    "        R-squared Score (R²): 0.76\n",
    "\n",
    "![img](./plots/residuals_distribution_2.png)\n",
    "![Feature Importance](./plots/feature_importance_2.png)\n",
    "![feature correlation heatmap](./plots/feature_correlation_heatmap_2.png)\n",
    "\n",
    "        \n",
    "next feature to remove, surprinsingly is education level wich has a low correlation with salary, but if pruned from the dataset the model shows performs worst than before with the same hyperparametrization.\n",
    "\n",
    "        Random Forest Regressor Performance After Hyperparameter Tuning:\n",
    "        Mean Squared Error (MSE): 808374516.33\n",
    "        Mean Absolute Error (MAE): 17240.95\n",
    "        R-squared Score (R²): 0.67\n",
    "\n",
    "![img](./plots/residuals_distribution_3.png)\n",
    "![Feature Importance](./plots/feature_importance_3.png)\n",
    "![feature correlation heatmap](./plots/feature_correlation_heatmap_3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this step we create a second model to test performance of different approaches in this specific problem. I'm using the same dataset and features as the random forest regressor but normalized using the stardad scaling between -1 and 1 wich provides better performances on NN sequiential models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = modeling.train_NN_model(normalized_X_train_nn,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the test dataset to predict salaries based on the trained model for a first fast evaluation.\n",
    "\n",
    "nn_model = keras.models.load_model('./models/neural_network_model.keras')\n",
    "\n",
    "evaluation.evaluate_NN_model(normalized_X_test_nn, y_test, nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = keras.models.load_model('./models/neural_network_model.keras')\n",
    "\n",
    "\n",
    "evaluation.calculate_metrics(normalized_X_test_nn, y_test, nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Comparison against dummy regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = keras.models.load_model('./models/neural_network_model.keras')\n",
    "dummy = joblib.load(open('./models/dummy_reggresor_model.pkl', 'rb'))\n",
    "rf_model = joblib.load(open('./models/random_forest_model.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "models_data = {\n",
    "    'Random Forest': (rf_model, normalized_X_test, y_test),\n",
    "    'Neural Network': (nn_model, normalized_X_test_nn, y_test),\n",
    "    'Dummy Regresson': (dummy, normalized_X_test, y_test)\n",
    "}\n",
    "\n",
    "comparison_results = model_compare.compare_models(models_data, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generation both aproaches for modeling the problem i settled on using the neural network approach since it seems to capture better the relationships between the features and is in general more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = inference.get_unique_job_titles(prefix=\"nn_\")\n",
    "form = create_input_form(job_titles)\n",
    "display(form)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
