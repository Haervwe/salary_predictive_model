{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 04:46:38.258265: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#necesary imports\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "fm._log.setLevel(logging.WARNING)\n",
    "from src import data_loading\n",
    "from src import preprocessing\n",
    "from src import visualize_data\n",
    "from src import feature_engenieering\n",
    "from src import modeling\n",
    "from src import evaluation\n",
    "from src import model_compare\n",
    "from src import inference\n",
    "from src.inference_jupyter_form import create_input_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary Predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## [1. Data Loading and Preprocessing](#1-data-loading-and-preprocessing)\n",
    "### [1.1. Loading Data](#11-loading-data)\n",
    "### [1.2. Data Cleansing and Imputation](#12-data-cleansing-and-imputation)\n",
    "### [1.3. Data Visualization](#13-data-visualization)\n",
    "\n",
    "## [2. Feature Engineering and Data Splitting](#2-feature-engineering-and-data-splitting)\n",
    "### [2.1. Splitting Data](#21-splitting-data)\n",
    "### [2.2. Data Normalization and Scaling](#22-data-normalization-and-scaling)\n",
    "\n",
    "## [3. Model Training and Evaluation](#3-model-training-and-evaluation)\n",
    "### [3.1. Baseline Model: Dummy Regressor](#31-baseline-model-dummy-regressor)\n",
    "### [3.2. Random Forest Regressor](#32-random-forest-regressor)\n",
    "### [3.3. RFR initial testing](#33-rfr-initial-testing)\n",
    "### [3.4. Feature Selection Process](#34-feature-selection-process)\n",
    "### [3.5. Neural Network Model](#35-neural-network-model)\n",
    "### [3.5. Neural Network Model initial testing](#35-neural-network-model-initial-testing)\n",
    "\n",
    "## [4. Model Comparison](#4-model-comparison)\n",
    "### [4.1. Final Model Selection](#41-final-model-selection)\n",
    "\n",
    "## [5. Inference](#5-inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Loading and preprocessing\n",
    "\n",
    "The data preprocessing pipeline consists of several key stages to prepare the dataset for modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading Data\n",
    "\n",
    "\n",
    "\n",
    " The `load_data` function in `data_loading.py` loads multiple CSV files and merges them into a single DataFrame using the 'id' column as the merging key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#files path for the raw dataset:\n",
    "\n",
    "data_files = ['./data/people.csv','./data/descriptions.csv','./data/salary.csv',]\n",
    "\n",
    "\n",
    "#merge datasets in a cohesive Dataframe\n",
    "\n",
    "full_dataset = data_loading.load_data(data_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data cleansing and imputation\n",
    "\n",
    "The `infer_missing_values_in_dataframe` function in `llm_dataset_filler.py` uses a local LLM to infer missing values for specific fields (e.g., Age, Gender, Education Level) based on the provided descriptions. It utilizes asynchronous calls to the LLM API and updates the DataFrame with inferred values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#preprocessing of the dataframe adds missing values with LLM inference over descriptions of each row, drops the incomplete rows and cleans up the data.\n",
    "\n",
    "cleansed_dataset = await preprocessing.preprocess(full_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Visualization\n",
    "\n",
    "Im using simple visualizations for the dataset in order to see if how the datset is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_data.visualize_dataset(cleansed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Feature engineering and data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Splitting data\n",
    "\n",
    "Dividing the dataset into training and testing subsets to evaluate model performance. This step ensures an unbiased assessment of the model's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#split the dataset into an 80 / 20 ratio for training and testing.\n",
    "\n",
    "X_train, X_test, y_train, y_test = feature_engenieering.split_data(cleansed_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Normalization and Scaling\n",
    "\n",
    " Normalizing and scaling data to standardize features, which helps improve model convergence and performance. unsing  Min-Max Scaling for the RNR model and Standardization for the NN model, the values for categorical features are treated case by case,for education its modeled as a linear relationship meaning 0 for bachellors m 1 for Masters and 2 for PHD, the Job title values since they have a strong correlation with salary but lack cardinality, the target encoder method seems to be the optimal. once the datasets are created are saved along with scalers and the job title target encoded table and their values in pkl format to be used later for training and inference respectibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#normalize and scale the datasets using MinMaxScaler and target encoder for random forest\n",
    "\n",
    "normalized_X_train, te, scaler = feature_engenieering.normalize_train_data(X_train, y_train,MinMaxScaler())\n",
    "\n",
    "normalized_X_test = feature_engenieering.normalize_test_data(X_test, te, scaler)\n",
    "\n",
    "\n",
    "#normalize and scale the datasets using MinMaxScaler and target encoder for Neural Networks\n",
    "\n",
    "normalized_X_train_nn, te_nn, scaler_nn = feature_engenieering.normalize_train_data(X_train, y_train,StandardScaler(),\"nn_\")\n",
    "\n",
    "normalized_X_test_nn = feature_engenieering.normalize_test_data(X_test, te_nn, scaler_nn)\n",
    "\n",
    "X_train.to_pickle(\"./data/X_train.pkl\")\n",
    "y_train.to_pickle(\"./data/y_train.pkl\")\n",
    "X_test.to_pickle(\"./data/X_test.pkl\")\n",
    "y_test.to_pickle(\"./data/y_test.pkl\")\n",
    "normalized_X_train.to_pickle(\"./data/normalized_X_train.pkl\")\n",
    "normalized_X_train_nn.to_pickle(\"./data/normalized_X_train_nn.pkl\")\n",
    "normalized_X_test.to_pickle(\"./data/normalized_X_test.pkl\")\n",
    "normalized_X_test_nn.to_pickle(\"./data/normalized_X_test_nn.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Model Training and Evaluation\n",
    "\n",
    "The first step is to load all the relevant files in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load generated datasets as pkl\n",
    "\n",
    "normalized_X_train = pd.read_pickle('./data/normalized_X_train.pkl')\n",
    "normalized_X_train_nn = pd.read_pickle('./data/normalized_X_train_nn.pkl')\n",
    "\n",
    "normalized_X_test = pd.read_pickle('./data/normalized_X_test.pkl')\n",
    "normalized_X_test_nn = pd.read_pickle('./data/normalized_X_test_nn.pkl')\n",
    "\n",
    "y_train = pd.read_pickle('./data/y_train.pkl')\n",
    "y_test = pd.read_pickle('./data/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Baseline Model: Dummy Regressor\n",
    "\n",
    "Establishing a baseline performance using a dummy regressor. This simple model provides a reference point for comparing advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the datasets created we use the train with MinMaxScaling dataset splits in a script to create a Random Forest Regressor using the scikit-learn framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traing a Dummy Reggressor to use as a baseline for model performance comparison and then train a Random Forest Regressor algorithm with hyperparameter tuning. We also evaluate the trained model by calculating metrics such as mean absolute error (MAE), root mean squared error (RMSE) and R-squared e (R2) and plot a scatterplot of predicted vs actual salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy = modeling.train_dummy_regressor(normalized_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest Regressor\n",
    "\n",
    "Training a Random Forest Regressor, a robust ensemble model, to predict target variables. This model aggregates results from multiple decision trees. this step also includes Optimizing model performance by tuning hyperparameters and selecting the most relevant features using a grid search technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a model using a random forest regressor algorithm and print out the predictions for the normalized test data.\n",
    "\n",
    "rf_model = modeling.train_model(normalized_X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 RFR initial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the test dataset to predict salaries based on the trained model for a first fast evaluation.\n",
    "\n",
    "\n",
    "rf_model = joblib.load(open('./models/random_forest_model.pkl', 'rb'))\n",
    "\n",
    "evaluation.evaluate_model(normalized_X_test, y_test, normalized_X_train,y_train, rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use Confidence intervals to further test the  models performance using bootstraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_model = joblib.load(open('./models/random_forest_model.pkl', 'rb'))\n",
    "\n",
    "evaluation.calculate_metrics(normalized_X_test, y_test, rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Feature selection process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose the Random Forest Reggresor to be able to catch non linear relationships better with realtively low data quantity for training, the model is Hyperparametrized using a grid approach and the best model is selected for testing.\n",
    "\n",
    "In the process of selecting features for the Random Forest Regressor model, based on correlation and feature importance analysis after the first \"naive\" model was trained in the normalized data as is to gather more information on the relationships and model error R-squared Score (R²).\n",
    "\n",
    "it is also quite notable that the heteroscedasticity present in the middle of the distribution seems to be caused by the dataset itself, most possibly by the lack of examples to fill the distribution appropiately, otherwise the distribution of residuals seems to be acceptable considering there are no statiscally significant outliers in the dataset.\n",
    "\n",
    "        Random Forest Regressor Performance After Hyperparameter Tuning:\n",
    "        Mean Squared Error (MSE): 866064920.96\n",
    "        Mean Absolute Error (MAE): 17982.33\n",
    "        R-squared Score (R²): 0.65\n",
    "        \n",
    "![img](./plots/residuals_distribution.png)\n",
    "![Feature Importance](./plots/feature_importance.png)\n",
    "![feature correlation heatmap](./plots/feature_correlation_heatmap.png)\n",
    "\n",
    "\n",
    "\n",
    "It can be seen now better results by droping geneder since it has a really low correlation with salary and importance , and lower the smoothing for the encoded job titles to 3 (10 was the previous value).\n",
    "\n",
    "        Random Forest Regressor Performance After Hyperparameter Tuning:\n",
    "        Mean Squared Error (MSE): 600148032.12\n",
    "        Mean Absolute Error (MAE): 14748.68\n",
    "        R-squared Score (R²): 0.76\n",
    "\n",
    "![img](./plots/residuals_distribution_2.png)\n",
    "![Feature Importance](./plots/feature_importance_2.png)\n",
    "![feature correlation heatmap](./plots/feature_correlation_heatmap_2.png)\n",
    "\n",
    "        \n",
    "next feature to remove, surprinsingly is education level wich has a low correlation with salary, but if pruned from the dataset the model shows performs worst than before with the same hyperparametrization.\n",
    "\n",
    "        Random Forest Regressor Performance After Hyperparameter Tuning:\n",
    "        Mean Squared Error (MSE): 808374516.33\n",
    "        Mean Absolute Error (MAE): 17240.95\n",
    "        R-squared Score (R²): 0.67\n",
    "\n",
    "![img](./plots/residuals_distribution_3.png)\n",
    "![Feature Importance](./plots/feature_importance_3.png)\n",
    "![feature correlation heatmap](./plots/feature_correlation_heatmap_3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this step we create a second model to test performance of different approaches in this specific problem. I'm using the same dataset and features as the random forest regressor but normalized using the stardad scaling between  wich provides better performances on NN sequiential models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 04:47:04.602724: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-17 04:47:07.006903: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-17 04:47:07.006975: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-17 04:47:07.008097: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-17 04:47:07.008184: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-17 04:47:07.008239: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-17 04:47:07.008322: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-17 04:47:07.008378: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-17 04:47:07.008434: I external/local_xla/xla/stream_executor/rocm/rocm_executor.cc:920] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-17 04:47:07.008450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15278 MB memory:  -> device: 0, name: AMD Radeon RX 6900 XT, pci bus id: 0000:0a:00.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737100028.529856  307456 service.cc:146] XLA service 0x740144005510 initialized for platform ROCM (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737100028.529890  307456 service.cc:154]   StreamExecutor device (0): AMD Radeon RX 6900 XT, AMDGPU ISA version: gfx1030\n",
      "2025-01-17 04:47:08.552662: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 4s/step - loss: 14108610560.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737100031.585426  307456 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 500ms/step - loss: 12576406528.0000 - val_loss: 12459951104.0000\n",
      "Epoch 2/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13129858048.0000 - val_loss: 12459877376.0000\n",
      "Epoch 3/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11716809728.0000 - val_loss: 12459784192.0000\n",
      "Epoch 4/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12423227392.0000 - val_loss: 12459642880.0000\n",
      "Epoch 5/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12377072640.0000 - val_loss: 12459426816.0000\n",
      "Epoch 6/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12169100288.0000 - val_loss: 12459084800.0000\n",
      "Epoch 7/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12920995840.0000 - val_loss: 12458563584.0000\n",
      "Epoch 8/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12853793792.0000 - val_loss: 12457793536.0000\n",
      "Epoch 9/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12233461760.0000 - val_loss: 12456679424.0000\n",
      "Epoch 10/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12134553600.0000 - val_loss: 12455062528.0000\n",
      "Epoch 11/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12832712704.0000 - val_loss: 12452801536.0000\n",
      "Epoch 12/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12683782144.0000 - val_loss: 12449666048.0000\n",
      "Epoch 13/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12596611072.0000 - val_loss: 12445411328.0000\n",
      "Epoch 14/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12823160832.0000 - val_loss: 12439812096.0000\n",
      "Epoch 15/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12055406592.0000 - val_loss: 12432602112.0000\n",
      "Epoch 16/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12821969920.0000 - val_loss: 12423326720.0000\n",
      "Epoch 17/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12655365120.0000 - val_loss: 12411731968.0000\n",
      "Epoch 18/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12421329920.0000 - val_loss: 12397465600.0000\n",
      "Epoch 19/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12151227392.0000 - val_loss: 12379988992.0000\n",
      "Epoch 20/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13018141696.0000 - val_loss: 12358457344.0000\n",
      "Epoch 21/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12147628032.0000 - val_loss: 12332911616.0000\n",
      "Epoch 22/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11201692672.0000 - val_loss: 12302790656.0000\n",
      "Epoch 23/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12374035456.0000 - val_loss: 12266771456.0000\n",
      "Epoch 24/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12287896576.0000 - val_loss: 12224670720.0000\n",
      "Epoch 25/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12783813632.0000 - val_loss: 12175827968.0000\n",
      "Epoch 26/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12575660032.0000 - val_loss: 12119605248.0000\n",
      "Epoch 27/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11900376064.0000 - val_loss: 12056529920.0000\n",
      "Epoch 28/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12678725632.0000 - val_loss: 11983634432.0000\n",
      "Epoch 29/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11756424192.0000 - val_loss: 11906084864.0000\n",
      "Epoch 30/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12438610944.0000 - val_loss: 11816336384.0000\n",
      "Epoch 31/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11744939008.0000 - val_loss: 11717220352.0000\n",
      "Epoch 32/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11629842432.0000 - val_loss: 11608077312.0000\n",
      "Epoch 33/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12352769024.0000 - val_loss: 11485705216.0000\n",
      "Epoch 34/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11375757312.0000 - val_loss: 11352912896.0000\n",
      "Epoch 35/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10580855808.0000 - val_loss: 11203282944.0000\n",
      "Epoch 36/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10783906816.0000 - val_loss: 11042318336.0000\n",
      "Epoch 37/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11496561664.0000 - val_loss: 10865380352.0000\n",
      "Epoch 38/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11020621824.0000 - val_loss: 10676547584.0000\n",
      "Epoch 39/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11274377216.0000 - val_loss: 10475287552.0000\n",
      "Epoch 40/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10432564224.0000 - val_loss: 10261981184.0000\n",
      "Epoch 41/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10379331584.0000 - val_loss: 10030282752.0000\n",
      "Epoch 42/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10021466112.0000 - val_loss: 9788435456.0000\n",
      "Epoch 43/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10155015168.0000 - val_loss: 9526051840.0000\n",
      "Epoch 44/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9926819840.0000 - val_loss: 9257449472.0000\n",
      "Epoch 45/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9466068992.0000 - val_loss: 8978589696.0000\n",
      "Epoch 46/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9063878656.0000 - val_loss: 8683228160.0000\n",
      "Epoch 47/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8904807424.0000 - val_loss: 8379279872.0000\n",
      "Epoch 48/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8659011584.0000 - val_loss: 8061724672.0000\n",
      "Epoch 49/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7990515200.0000 - val_loss: 7746985472.0000\n",
      "Epoch 50/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7829146624.0000 - val_loss: 7413451264.0000\n",
      "Epoch 51/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7623631360.0000 - val_loss: 7078861312.0000\n",
      "Epoch 52/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7126422016.0000 - val_loss: 6731785728.0000\n",
      "Epoch 53/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6417490944.0000 - val_loss: 6390234624.0000\n",
      "Epoch 54/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6531043328.0000 - val_loss: 6047580672.0000\n",
      "Epoch 55/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6093846528.0000 - val_loss: 5704857088.0000\n",
      "Epoch 56/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6065141760.0000 - val_loss: 5359785472.0000\n",
      "Epoch 57/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5541236736.0000 - val_loss: 5031741440.0000\n",
      "Epoch 58/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5462336000.0000 - val_loss: 4712795136.0000\n",
      "Epoch 59/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4993849344.0000 - val_loss: 4416756224.0000\n",
      "Epoch 60/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4432640000.0000 - val_loss: 4123804160.0000\n",
      "Epoch 61/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4529948160.0000 - val_loss: 3846673664.0000\n",
      "Epoch 62/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3862393344.0000 - val_loss: 3582977792.0000\n",
      "Epoch 63/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3901657088.0000 - val_loss: 3329428224.0000\n",
      "Epoch 64/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3314910976.0000 - val_loss: 3096716288.0000\n",
      "Epoch 65/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3317890816.0000 - val_loss: 2890211072.0000\n",
      "Epoch 66/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3351585792.0000 - val_loss: 2695308800.0000\n",
      "Epoch 67/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3220591872.0000 - val_loss: 2526164224.0000\n",
      "Epoch 68/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2912601600.0000 - val_loss: 2372670720.0000\n",
      "Epoch 69/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2474236160.0000 - val_loss: 2241424896.0000\n",
      "Epoch 70/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2589711872.0000 - val_loss: 2123702272.0000\n",
      "Epoch 71/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2336644096.0000 - val_loss: 2023387648.0000\n",
      "Epoch 72/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2310793472.0000 - val_loss: 1939616896.0000\n",
      "Epoch 73/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2134432128.0000 - val_loss: 1870155648.0000\n",
      "Epoch 74/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2262913792.0000 - val_loss: 1805813376.0000\n",
      "Epoch 75/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1978172928.0000 - val_loss: 1757115904.0000\n",
      "Epoch 76/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2105467904.0000 - val_loss: 1716706688.0000\n",
      "Epoch 77/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1917498880.0000 - val_loss: 1682376576.0000\n",
      "Epoch 78/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1740528000.0000 - val_loss: 1653075712.0000\n",
      "Epoch 79/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1814321920.0000 - val_loss: 1625386112.0000\n",
      "Epoch 80/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1696119936.0000 - val_loss: 1601618048.0000\n",
      "Epoch 81/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1575262208.0000 - val_loss: 1582732032.0000\n",
      "Epoch 82/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1697484544.0000 - val_loss: 1565298688.0000\n",
      "Epoch 83/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1617665152.0000 - val_loss: 1549053696.0000\n",
      "Epoch 84/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1515683200.0000 - val_loss: 1535044224.0000\n",
      "Epoch 85/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1587442560.0000 - val_loss: 1521979392.0000\n",
      "Epoch 86/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1577962368.0000 - val_loss: 1509251072.0000\n",
      "Epoch 87/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1755865216.0000 - val_loss: 1497345280.0000\n",
      "Epoch 88/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1627650304.0000 - val_loss: 1485610752.0000\n",
      "Epoch 89/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1556364928.0000 - val_loss: 1474875776.0000\n",
      "Epoch 90/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1624948736.0000 - val_loss: 1463181440.0000\n",
      "Epoch 91/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1492468224.0000 - val_loss: 1450466048.0000\n",
      "Epoch 92/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1544006528.0000 - val_loss: 1439626496.0000\n",
      "Epoch 93/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1434390656.0000 - val_loss: 1428127488.0000\n",
      "Epoch 94/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1727152128.0000 - val_loss: 1414141952.0000\n",
      "Epoch 95/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1376289408.0000 - val_loss: 1404180864.0000\n",
      "Epoch 96/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1472619392.0000 - val_loss: 1395365760.0000\n",
      "Epoch 97/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1437210368.0000 - val_loss: 1384526208.0000\n",
      "Epoch 98/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1491935232.0000 - val_loss: 1369528320.0000\n",
      "Epoch 99/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1401992448.0000 - val_loss: 1356748544.0000\n",
      "Epoch 100/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1328723456.0000 - val_loss: 1345086720.0000\n",
      "Epoch 101/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1412414464.0000 - val_loss: 1331356160.0000\n",
      "Epoch 102/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1323686784.0000 - val_loss: 1316646016.0000\n",
      "Epoch 103/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1259378944.0000 - val_loss: 1304042624.0000\n",
      "Epoch 104/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1356058496.0000 - val_loss: 1293341568.0000\n",
      "Epoch 105/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1242666752.0000 - val_loss: 1282284160.0000\n",
      "Epoch 106/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1287561856.0000 - val_loss: 1271833088.0000\n",
      "Epoch 107/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1271832320.0000 - val_loss: 1260440704.0000\n",
      "Epoch 108/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1262304384.0000 - val_loss: 1247965696.0000\n",
      "Epoch 109/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1356443136.0000 - val_loss: 1237900544.0000\n",
      "Epoch 110/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1120176640.0000 - val_loss: 1227828992.0000\n",
      "Epoch 111/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1196187776.0000 - val_loss: 1217420928.0000\n",
      "Epoch 112/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1262880000.0000 - val_loss: 1204661376.0000\n",
      "Epoch 113/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1143247872.0000 - val_loss: 1194105216.0000\n",
      "Epoch 114/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1094688128.0000 - val_loss: 1181238656.0000\n",
      "Epoch 115/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1160964608.0000 - val_loss: 1169951360.0000\n",
      "Epoch 116/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1264858752.0000 - val_loss: 1158363264.0000\n",
      "Epoch 117/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1212052480.0000 - val_loss: 1147163904.0000\n",
      "Epoch 118/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1146202624.0000 - val_loss: 1137140992.0000\n",
      "Epoch 119/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1290741504.0000 - val_loss: 1123491200.0000\n",
      "Epoch 120/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1121716224.0000 - val_loss: 1111309696.0000\n",
      "Epoch 121/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1143218688.0000 - val_loss: 1100154368.0000\n",
      "Epoch 122/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1164044288.0000 - val_loss: 1090553856.0000\n",
      "Epoch 123/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1108034816.0000 - val_loss: 1082680192.0000\n",
      "Epoch 124/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 941398400.0000 - val_loss: 1072407168.0000\n",
      "Epoch 125/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 949465728.0000 - val_loss: 1062258432.0000\n",
      "Epoch 126/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 963920960.0000 - val_loss: 1051601920.0000\n",
      "Epoch 127/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 999343424.0000 - val_loss: 1039417920.0000\n",
      "Epoch 128/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 965050240.0000 - val_loss: 1028721920.0000\n",
      "Epoch 129/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1081325056.0000 - val_loss: 1017981760.0000\n",
      "Epoch 130/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 900446080.0000 - val_loss: 1006712896.0000\n",
      "Epoch 131/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1028603264.0000 - val_loss: 995239360.0000\n",
      "Epoch 132/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 868576640.0000 - val_loss: 983938368.0000\n",
      "Epoch 133/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1050082432.0000 - val_loss: 974228672.0000\n",
      "Epoch 134/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 972769664.0000 - val_loss: 964475520.0000\n",
      "Epoch 135/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 881155008.0000 - val_loss: 954747648.0000\n",
      "Epoch 136/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1016534720.0000 - val_loss: 942709760.0000\n",
      "Epoch 137/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 922546112.0000 - val_loss: 933001408.0000\n",
      "Epoch 138/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 884633344.0000 - val_loss: 923587584.0000\n",
      "Epoch 139/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 811320192.0000 - val_loss: 914970688.0000\n",
      "Epoch 140/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 917803520.0000 - val_loss: 904816704.0000\n",
      "Epoch 141/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 923099008.0000 - val_loss: 895227264.0000\n",
      "Epoch 142/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 887688448.0000 - val_loss: 886003840.0000\n",
      "Epoch 143/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 837350016.0000 - val_loss: 877733440.0000\n",
      "Epoch 144/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 912215872.0000 - val_loss: 866634048.0000\n",
      "Epoch 145/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 798045376.0000 - val_loss: 856984704.0000\n",
      "Epoch 146/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 654726912.0000 - val_loss: 848037888.0000\n",
      "Epoch 147/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 817888896.0000 - val_loss: 839446080.0000\n",
      "Epoch 148/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 827270144.0000 - val_loss: 827843968.0000\n",
      "Epoch 149/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 767851328.0000 - val_loss: 815926592.0000\n",
      "Epoch 150/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 722848128.0000 - val_loss: 806370624.0000\n",
      "Epoch 151/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 691264320.0000 - val_loss: 797140736.0000\n",
      "Epoch 152/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 765060928.0000 - val_loss: 788475648.0000\n",
      "Epoch 153/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 690929280.0000 - val_loss: 781395072.0000\n",
      "Epoch 154/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 715299072.0000 - val_loss: 772178368.0000\n",
      "Epoch 155/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 706353472.0000 - val_loss: 764789952.0000\n",
      "Epoch 156/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 645053376.0000 - val_loss: 758228352.0000\n",
      "Epoch 157/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 683626560.0000 - val_loss: 750762880.0000\n",
      "Epoch 158/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 680862720.0000 - val_loss: 743912128.0000\n",
      "Epoch 159/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 676242304.0000 - val_loss: 735597504.0000\n",
      "Epoch 160/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 708203008.0000 - val_loss: 727200000.0000\n",
      "Epoch 161/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 729602368.0000 - val_loss: 714921344.0000\n",
      "Epoch 162/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 743017216.0000 - val_loss: 703261760.0000\n",
      "Epoch 163/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 751376384.0000 - val_loss: 693813632.0000\n",
      "Epoch 164/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 607210688.0000 - val_loss: 683905792.0000\n",
      "Epoch 165/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 610615552.0000 - val_loss: 675604224.0000\n",
      "Epoch 166/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 670424576.0000 - val_loss: 669421760.0000\n",
      "Epoch 167/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 697928256.0000 - val_loss: 661402624.0000\n",
      "Epoch 168/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 670389504.0000 - val_loss: 654910144.0000\n",
      "Epoch 169/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 602038144.0000 - val_loss: 649830656.0000\n",
      "Epoch 170/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 592374080.0000 - val_loss: 642708160.0000\n",
      "Epoch 171/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 618088448.0000 - val_loss: 634773888.0000\n",
      "Epoch 172/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 664085824.0000 - val_loss: 624960256.0000\n",
      "Epoch 173/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 611463040.0000 - val_loss: 616403520.0000\n",
      "Epoch 174/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 580128576.0000 - val_loss: 611299264.0000\n",
      "Epoch 175/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 583563648.0000 - val_loss: 606164160.0000\n",
      "Epoch 176/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 518210720.0000 - val_loss: 600751872.0000\n",
      "Epoch 177/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 613810176.0000 - val_loss: 592158336.0000\n",
      "Epoch 178/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 538712256.0000 - val_loss: 584032960.0000\n",
      "Epoch 179/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 668109568.0000 - val_loss: 575582464.0000\n",
      "Epoch 180/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 505339712.0000 - val_loss: 570464128.0000\n",
      "Epoch 181/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 559941376.0000 - val_loss: 563972800.0000\n",
      "Epoch 182/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 538380864.0000 - val_loss: 559110016.0000\n",
      "Epoch 183/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 517066080.0000 - val_loss: 553688576.0000\n",
      "Epoch 184/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 558539008.0000 - val_loss: 547112768.0000\n",
      "Epoch 185/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 513908160.0000 - val_loss: 538714048.0000\n",
      "Epoch 186/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 486022048.0000 - val_loss: 534831744.0000\n",
      "Epoch 187/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 517556768.0000 - val_loss: 530165056.0000\n",
      "Epoch 188/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 566641280.0000 - val_loss: 522730944.0000\n",
      "Epoch 189/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 528283232.0000 - val_loss: 518172672.0000\n",
      "Epoch 190/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 475399008.0000 - val_loss: 512406112.0000\n",
      "Epoch 191/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 538407936.0000 - val_loss: 505183776.0000\n",
      "Epoch 192/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 517132192.0000 - val_loss: 495009856.0000\n",
      "Epoch 193/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 557449216.0000 - val_loss: 488199104.0000\n",
      "Epoch 194/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 517275712.0000 - val_loss: 483214624.0000\n",
      "Epoch 195/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 563290496.0000 - val_loss: 478656512.0000\n",
      "Epoch 196/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 541198208.0000 - val_loss: 474966656.0000\n",
      "Epoch 197/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 499303968.0000 - val_loss: 470508000.0000\n",
      "Epoch 198/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 438202720.0000 - val_loss: 465026496.0000\n",
      "Epoch 199/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 450423296.0000 - val_loss: 461517888.0000\n",
      "Epoch 200/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 469438368.0000 - val_loss: 457692704.0000\n",
      "Epoch 201/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 491268704.0000 - val_loss: 453724256.0000\n",
      "Epoch 202/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 537545344.0000 - val_loss: 449308128.0000\n",
      "Epoch 203/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 524810336.0000 - val_loss: 442734816.0000\n",
      "Epoch 204/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 527655200.0000 - val_loss: 437380256.0000\n",
      "Epoch 205/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 472323104.0000 - val_loss: 432345600.0000\n",
      "Epoch 206/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 529148608.0000 - val_loss: 426417088.0000\n",
      "Epoch 207/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 471805568.0000 - val_loss: 421142304.0000\n",
      "Epoch 208/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 439222368.0000 - val_loss: 417020992.0000\n",
      "Epoch 209/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 534959104.0000 - val_loss: 414669952.0000\n",
      "Epoch 210/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 364161888.0000 - val_loss: 411885376.0000\n",
      "Epoch 211/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 439780608.0000 - val_loss: 408531776.0000\n",
      "Epoch 212/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 463743360.0000 - val_loss: 403946688.0000\n",
      "Epoch 213/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 466372576.0000 - val_loss: 399659552.0000\n",
      "Epoch 214/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 436977632.0000 - val_loss: 396517504.0000\n",
      "Epoch 215/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 525065824.0000 - val_loss: 391427488.0000\n",
      "Epoch 216/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 515048608.0000 - val_loss: 386386464.0000\n",
      "Epoch 217/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 423851296.0000 - val_loss: 382266752.0000\n",
      "Epoch 218/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 466440000.0000 - val_loss: 377782560.0000\n",
      "Epoch 219/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 397263648.0000 - val_loss: 375000256.0000\n",
      "Epoch 220/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 461612768.0000 - val_loss: 370558752.0000\n",
      "Epoch 221/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 375142976.0000 - val_loss: 367884480.0000\n",
      "Epoch 222/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 448832416.0000 - val_loss: 364361280.0000\n",
      "Epoch 223/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 436028384.0000 - val_loss: 359407584.0000\n",
      "Epoch 224/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 480761472.0000 - val_loss: 356101632.0000\n",
      "Epoch 225/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 400916384.0000 - val_loss: 350321280.0000\n",
      "Epoch 226/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 457903488.0000 - val_loss: 345794976.0000\n",
      "Epoch 227/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 433386016.0000 - val_loss: 343088256.0000\n",
      "Epoch 228/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 470111616.0000 - val_loss: 340290016.0000\n",
      "Epoch 229/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 447252128.0000 - val_loss: 337003968.0000\n",
      "Epoch 230/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 405793280.0000 - val_loss: 334423136.0000\n",
      "Epoch 231/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 482393152.0000 - val_loss: 332188352.0000\n",
      "Epoch 232/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 431637408.0000 - val_loss: 327926144.0000\n",
      "Epoch 233/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 404826368.0000 - val_loss: 324742080.0000\n",
      "Epoch 234/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 479414144.0000 - val_loss: 323544160.0000\n",
      "Epoch 235/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 475683904.0000 - val_loss: 319939712.0000\n",
      "Epoch 236/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 405163744.0000 - val_loss: 317404704.0000\n",
      "Epoch 237/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 377536736.0000 - val_loss: 311519104.0000\n",
      "Epoch 238/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 367427232.0000 - val_loss: 310214176.0000\n",
      "Epoch 239/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 444750976.0000 - val_loss: 308349280.0000\n",
      "Epoch 240/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 390266208.0000 - val_loss: 304473536.0000\n",
      "Epoch 241/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 329337792.0000 - val_loss: 300301952.0000\n",
      "Epoch 242/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 451903008.0000 - val_loss: 296641568.0000\n",
      "Epoch 243/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 389650816.0000 - val_loss: 292927680.0000\n",
      "Epoch 244/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 371553984.0000 - val_loss: 289134784.0000\n",
      "Epoch 245/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 422302176.0000 - val_loss: 285655136.0000\n",
      "Epoch 246/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 369387872.0000 - val_loss: 282730016.0000\n",
      "Epoch 247/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 397062752.0000 - val_loss: 279961472.0000\n",
      "Epoch 248/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 373376064.0000 - val_loss: 277985056.0000\n",
      "Epoch 249/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 400422528.0000 - val_loss: 277123488.0000\n",
      "Epoch 250/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 424292160.0000 - val_loss: 274155360.0000\n",
      "Epoch 251/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 346299872.0000 - val_loss: 272406400.0000\n",
      "Epoch 252/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 337957536.0000 - val_loss: 271456928.0000\n",
      "Epoch 253/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 287333376.0000 - val_loss: 271026176.0000\n",
      "Epoch 254/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 344137760.0000 - val_loss: 270562240.0000\n",
      "Epoch 255/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 368541280.0000 - val_loss: 269962112.0000\n",
      "Epoch 256/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 408214848.0000 - val_loss: 268286064.0000\n",
      "Epoch 257/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 314437984.0000 - val_loss: 266724864.0000\n",
      "Epoch 258/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 312484896.0000 - val_loss: 267036192.0000\n",
      "Epoch 259/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 386056640.0000 - val_loss: 267328544.0000\n",
      "Epoch 260/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 463940224.0000 - val_loss: 264884848.0000\n",
      "Epoch 261/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 315467648.0000 - val_loss: 261809216.0000\n",
      "Epoch 262/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 361768928.0000 - val_loss: 259036880.0000\n",
      "Epoch 263/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 350720992.0000 - val_loss: 257449760.0000\n",
      "Epoch 264/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 351465152.0000 - val_loss: 255460560.0000\n",
      "Epoch 265/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 411456832.0000 - val_loss: 254136304.0000\n",
      "Epoch 266/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 353666496.0000 - val_loss: 251310320.0000\n",
      "Epoch 267/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 400256480.0000 - val_loss: 249783104.0000\n",
      "Epoch 268/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 435028800.0000 - val_loss: 248957152.0000\n",
      "Epoch 269/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 328685568.0000 - val_loss: 246123120.0000\n",
      "Epoch 270/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 364971776.0000 - val_loss: 245554384.0000\n",
      "Epoch 271/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 346556608.0000 - val_loss: 244106832.0000\n",
      "Epoch 272/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 313056032.0000 - val_loss: 242228208.0000\n",
      "Epoch 273/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 371547104.0000 - val_loss: 240044880.0000\n",
      "Epoch 274/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 291555104.0000 - val_loss: 238410816.0000\n",
      "Epoch 275/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 351255328.0000 - val_loss: 237326656.0000\n",
      "Epoch 276/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 401957568.0000 - val_loss: 236084576.0000\n",
      "Epoch 277/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 332147968.0000 - val_loss: 235696736.0000\n",
      "Epoch 278/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 414054368.0000 - val_loss: 235718096.0000\n",
      "Epoch 279/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 343277632.0000 - val_loss: 234669664.0000\n",
      "Epoch 280/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 335889248.0000 - val_loss: 234190624.0000\n",
      "Epoch 281/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 294437280.0000 - val_loss: 232934896.0000\n",
      "Epoch 282/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 366336064.0000 - val_loss: 232459664.0000\n",
      "Epoch 283/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 412339872.0000 - val_loss: 231407456.0000\n",
      "Epoch 284/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 327103744.0000 - val_loss: 230775552.0000\n",
      "Epoch 285/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 379306304.0000 - val_loss: 229575264.0000\n",
      "Epoch 286/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 365628800.0000 - val_loss: 229050960.0000\n",
      "Epoch 287/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 321578400.0000 - val_loss: 227765776.0000\n",
      "Epoch 288/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 303855392.0000 - val_loss: 226915184.0000\n",
      "Epoch 289/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 323506400.0000 - val_loss: 225135248.0000\n",
      "Epoch 290/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 352739584.0000 - val_loss: 225063264.0000\n",
      "Epoch 291/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 288783264.0000 - val_loss: 224836096.0000\n",
      "Epoch 292/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 361782528.0000 - val_loss: 222008304.0000\n",
      "Epoch 293/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 311336064.0000 - val_loss: 219784960.0000\n",
      "Epoch 294/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 352643392.0000 - val_loss: 219452320.0000\n",
      "Epoch 295/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 336900160.0000 - val_loss: 219818464.0000\n",
      "Epoch 296/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 334284768.0000 - val_loss: 221286816.0000\n",
      "Epoch 297/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 414253440.0000 - val_loss: 221162304.0000\n",
      "Epoch 298/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 343634944.0000 - val_loss: 220378480.0000\n",
      "Epoch 299/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 367943296.0000 - val_loss: 217077712.0000\n",
      "Epoch 300/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 297712448.0000 - val_loss: 215395312.0000\n",
      "Epoch 301/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 316197056.0000 - val_loss: 213605232.0000\n",
      "Epoch 302/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 271709632.0000 - val_loss: 213946512.0000\n",
      "Epoch 303/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 332886464.0000 - val_loss: 217044784.0000\n",
      "Epoch 304/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 349860096.0000 - val_loss: 219450736.0000\n",
      "Epoch 305/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 267383648.0000 - val_loss: 220200528.0000\n",
      "Epoch 306/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 255421296.0000 - val_loss: 219501232.0000\n",
      "Epoch 307/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 309904576.0000 - val_loss: 216887696.0000\n",
      "Epoch 308/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 340682432.0000 - val_loss: 214903408.0000\n",
      "Epoch 309/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 321756928.0000 - val_loss: 212478736.0000\n",
      "Epoch 310/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 333073120.0000 - val_loss: 210629872.0000\n",
      "Epoch 311/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 266227904.0000 - val_loss: 211305888.0000\n",
      "Epoch 312/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 287782624.0000 - val_loss: 214025440.0000\n",
      "Epoch 313/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 382483264.0000 - val_loss: 216000352.0000\n",
      "Epoch 314/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 335411840.0000 - val_loss: 213932096.0000\n",
      "Epoch 315/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 346323776.0000 - val_loss: 212258384.0000\n",
      "Epoch 316/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 372020416.0000 - val_loss: 210162880.0000\n",
      "Epoch 317/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 340633568.0000 - val_loss: 208853200.0000\n",
      "Epoch 318/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 316004736.0000 - val_loss: 206847952.0000\n",
      "Epoch 319/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 344172672.0000 - val_loss: 204410528.0000\n",
      "Epoch 320/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 287262560.0000 - val_loss: 203845456.0000\n",
      "Epoch 321/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 376841440.0000 - val_loss: 203943760.0000\n",
      "Epoch 322/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 334047872.0000 - val_loss: 204212208.0000\n",
      "Epoch 323/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 341862368.0000 - val_loss: 206360528.0000\n",
      "Epoch 324/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 381939264.0000 - val_loss: 210014176.0000\n",
      "Epoch 325/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 335486688.0000 - val_loss: 208206512.0000\n",
      "Epoch 326/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 307570720.0000 - val_loss: 206696256.0000\n",
      "Epoch 327/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 317073216.0000 - val_loss: 205377504.0000\n",
      "Epoch 328/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 277237888.0000 - val_loss: 205587568.0000\n",
      "Epoch 329/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 330285216.0000 - val_loss: 203950672.0000\n",
      "Epoch 330/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 387994240.0000 - val_loss: 203023664.0000\n",
      "Epoch 331/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 292075136.0000 - val_loss: 202744512.0000\n",
      "Epoch 332/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 311128768.0000 - val_loss: 203115712.0000\n",
      "Epoch 333/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 256369392.0000 - val_loss: 206454464.0000\n",
      "Epoch 334/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 251413408.0000 - val_loss: 205366928.0000\n",
      "Epoch 335/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 250041568.0000 - val_loss: 202939232.0000\n",
      "Epoch 336/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 303241440.0000 - val_loss: 202563776.0000\n",
      "Epoch 337/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 363457376.0000 - val_loss: 200964928.0000\n",
      "Epoch 338/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 321369184.0000 - val_loss: 198768496.0000\n",
      "Epoch 339/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 311791040.0000 - val_loss: 196881520.0000\n",
      "Epoch 340/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 373473344.0000 - val_loss: 195756448.0000\n",
      "Epoch 341/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 297721792.0000 - val_loss: 194909920.0000\n",
      "Epoch 342/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 342196160.0000 - val_loss: 195118304.0000\n",
      "Epoch 343/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 365165120.0000 - val_loss: 196131296.0000\n",
      "Epoch 344/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 298558912.0000 - val_loss: 199362752.0000\n",
      "Epoch 345/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 313474208.0000 - val_loss: 202711216.0000\n",
      "Epoch 346/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 353544224.0000 - val_loss: 202017344.0000\n",
      "Epoch 347/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 292504064.0000 - val_loss: 199366704.0000\n",
      "Epoch 348/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 294744032.0000 - val_loss: 195611808.0000\n",
      "Epoch 349/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 299503744.0000 - val_loss: 193276112.0000\n",
      "Epoch 350/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 292173600.0000 - val_loss: 192306528.0000\n",
      "Epoch 351/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 363902208.0000 - val_loss: 192249744.0000\n",
      "Epoch 352/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 306038016.0000 - val_loss: 192781504.0000\n",
      "Epoch 353/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 246304672.0000 - val_loss: 193625872.0000\n",
      "Epoch 354/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 360290720.0000 - val_loss: 194407136.0000\n",
      "Epoch 355/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 364878688.0000 - val_loss: 194603984.0000\n",
      "Epoch 356/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 345511360.0000 - val_loss: 195392528.0000\n",
      "Epoch 357/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 341078048.0000 - val_loss: 197916944.0000\n",
      "Epoch 358/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 282448224.0000 - val_loss: 198596896.0000\n",
      "Epoch 359/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 333534912.0000 - val_loss: 199042080.0000\n",
      "Epoch 360/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 305835616.0000 - val_loss: 199580960.0000\n",
      "Epoch 361/1000\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 320056032.0000 - val_loss: 198397712.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 5 to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss plot saved to ./plots/nn_training_loss.png\n",
      "Neural Network model saved to ./models/neural_network_model.keras\n"
     ]
    }
   ],
   "source": [
    "modeling.train_NN_model(normalized_X_train_nn,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Neural Network Model initial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the test dataset to predict salaries based on the trained model for a first fast evaluation.\n",
    "\n",
    "nn_model = keras.models.load_model('./models/neural_network_model.keras')\n",
    "\n",
    "evaluation.evaluate_NN_model(normalized_X_test_nn, y_test,normalized_X_train_nn, nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = keras.models.load_model('./models/neural_network_model.keras')\n",
    "\n",
    "\n",
    "evaluation.calculate_metrics(normalized_X_test_nn, y_test, nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model Comparison\n",
    "Comparing the performance of all models using metrics such as accuracy, mean squared error (MSE), and R-squared to determine the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = keras.models.load_model('./models/neural_network_model.keras')\n",
    "dummy = joblib.load(open('./models/dummy_reggresor_model.pkl', 'rb'))\n",
    "rf_model = joblib.load(open('./models/random_forest_model.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "models_data = {\n",
    "    'Random Forest': (rf_model, normalized_X_test, y_test),\n",
    "    'Neural Network': (nn_model, normalized_X_test_nn, y_test),\n",
    "    'Dummy Regresson': (dummy, normalized_X_test, y_test)\n",
    "}\n",
    "\n",
    "comparison_results = model_compare.compare_models(models_data, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generation both aproaches for modeling the problem i settled on using the neural network approach since it seems to capture better the relationships between the features and is in general more precise.\n",
    "\n",
    "we can observe the comparisons on the following charts:\n",
    "\n",
    "![img](./plots/predicted_vs_actual_values_model_comparison.png)\n",
    "\n",
    "\n",
    "![img](./plots/error_model_comparison.png)\n",
    "\n",
    "\n",
    "![img](./plots/residuals_distribution_model_comparison.png)\n",
    "\n",
    "\n",
    "And the reported confidence intervals with bootstraping for each model: \n",
    "\n",
    "### **Model Performance Summary with Confidence Intervals**:\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "**Random Forest**:\n",
    "\n",
    "MSE: 591312507.301 (95% CI: [266726953.202, 1092637456.989])\n",
    "\n",
    "MAE: 14592.480 (95% CI: [10587.479, 19339.949])\n",
    "\n",
    "R2: 0.761 (95% CI: [0.635, 0.872])\n",
    "\n",
    "**Neural Network**:\n",
    "\n",
    "MSE: 451365450.081 (95% CI: [193911315.376, 891479517.642])\n",
    "\n",
    "MAE: 13879.159 (95% CI: [10770.149, 17989.091])\n",
    "\n",
    "R2: 0.820 (95% CI: [0.713, 0.904])\n",
    "\n",
    "**Dummy Regresson**:\n",
    "\n",
    "MSE: 2461198619.228 (95% CI: [1834308694.379, 3231299942.415])\n",
    "\n",
    "MAE: 41588.630 (95% CI: [35333.317, 48193.295])\n",
    "\n",
    "R2: -0.015 (95% CI: [-0.080, -0.000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Inference\n",
    "\n",
    "Using the best-performing model to make predictions on new or unseen data. This step involves applying the trained model to real-world scenarios or test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = inference.get_unique_job_titles(prefix=\"nn_\")\n",
    "form = create_input_form(job_titles)\n",
    "display(form)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
