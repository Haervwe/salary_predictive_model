{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Salary Predictive Model.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents:**\n",
    "\n",
    "---\n",
    "\n",
    "### [1 Imports Data Loading and Preprocessing](#1) \n",
    "- #### [1.1 Imports](#11)\n",
    "- #### [1.2 Loading Data](#12)\n",
    "- #### [1.3 Data Cleansing and Imputation](#13)\n",
    "- #### [1.4 Data Visualization](#14)\n",
    "- #### [1.5 Data Visualization Results](#15)\n",
    "\n",
    "### [2 Feature Engineering and Data Splitting](#2)\n",
    "- #### [2.1 Splitting Data](#21)\n",
    "- #### [2.2 Data Normalization and Scaling](#22)\n",
    "\n",
    "### [3 Model Training and Evaluation](#3)\n",
    "- #### [3.1 Baseline Model: Dummy Regressor](#31)\n",
    "- #### [3.2 Random Forest Regressor](#32)\n",
    "- #### [3.3 RFR Initial Testing](#33)\n",
    "- #### [3.4 Feature Selection Process](#34)\n",
    "- #### [3.5 Neural Network Model](#35)\n",
    "- #### [3.6 Neural Network Model Initial Testing](#36)\n",
    "\n",
    "### [4 Model Comparison](#4)\n",
    "- #### [4.1 Final Model Selection](#41)\n",
    "\n",
    "### [5 Inference](#5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    "# **1 Imports, Data Loading and preprocessing**\n",
    "\n",
    "The data preprocessing pipeline consists of several key stages to prepare the dataset for modeling:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a> \n",
    "## **1.1 Imports**\n",
    "\n",
    "All the necesary imports for the notebook to function properly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necesary imports\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler ,RobustScaler\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "fm._log.setLevel(logging.WARNING)\n",
    "from src import data_loading\n",
    "from src import preprocessing\n",
    "from src import visualize_data\n",
    "from src import feature_engenieering\n",
    "from src import modeling\n",
    "from src import evaluation\n",
    "from src import model_compare\n",
    "from src import inference\n",
    "from src.inference_jupyter_form import create_input_form\n",
    "from src import feature_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a>\n",
    "## **1.2 Loading Data**\n",
    "\n",
    "\n",
    "\n",
    " The `load_data` function in `data_loading.py` loads multiple CSV files and merges them into a single DataFrame using the 'id' column as the merging key.\n",
    "\n",
    "\n",
    " ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#files path for the raw dataset:\n",
    "\n",
    "data_files = ['./data/people.csv','./data/descriptions.csv','./data/salary.csv',]\n",
    "\n",
    "\n",
    "#merge datasets in a cohesive Dataframe\n",
    "\n",
    "full_dataset = data_loading.load_data(data_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"13\"></a>\n",
    "## **1.3 Data cleansing and imputation**\n",
    "\n",
    "In this step, I use the IQR metric to detect statistical outliers and also remove duplicate entries as well as using the descriptions for missing entries to fill them.\n",
    "\n",
    "The `infer_missing_values_in_dataframe` function in `llm_dataset_filler.py` leverages a local LLM to infer missing values for specific fields (e.g., Age, Gender, Education Level) based on the provided descriptions. It utilizes asynchronous calls to the LLM API and updates the DataFrame with inferred values.\n",
    "\n",
    "Additionally, I use IQR metrics to detect statistical outliers and remove duplicate entries to avoid overfitting on particular data points. It's important to note that while the functionality is implemented, the dataset does not contain duplicates or significant outliers detectable with this methodology.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_URL = 'http://localhost:11434'  # Replace with your actual server URL and port acceping any OpenAI compatible API endpoint\n",
    "MODEL_NAME = 'hermes3:8b-llama3.1-q6_K'       # Replace with your model name\n",
    "API_KEY = \"\"  # Replace with your API key\n",
    "\n",
    "cleansed_dataset = await preprocessing.preprocess(full_dataset, BASE_URL, MODEL_NAME, API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"14\"></a> \n",
    "## **1.4 Data Visualization**\n",
    "\n",
    "Im using simple visualizations for the dataset in order to see if how the datset is distributed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_data.visualize_dataset(cleansed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "<a id=\"15\"></a> \n",
    "\n",
    "### **1.5 Data Visualization Results**\n",
    "\n",
    "\n",
    "---\n",
    "#### **Scatter Plots**\n",
    "\n",
    "The following scatter plots visualize the relationships between different pairs of numerical variables in the dataset:\n",
    "\n",
    "1. **Scatter Plot of Salary vs Age**\n",
    "\n",
    "\n",
    "    ![Scatter Plot of Salary vs Age](./plots/scatter_Salary_vs_Age.png)\n",
    "\n",
    "    - This plot shows the relationship between Salary and Age. shows a good linear relationship with more variance at the end of the distribution, making it a strong feature for the future model.\n",
    "\n",
    "\n",
    "2. **Scatter Plot of Salary vs Years of Experience**\n",
    "\n",
    "    ![Scatter Plot of Salary vs Years of Experience](./plots/scatter_Salary_vs_Years_of_Experience.png)\n",
    "\n",
    "    - This plot illustrates the relationship between Salary and Years of Experience.shows a good linear relationship similar to Age.\n",
    "\n",
    "\n",
    "3. **Scatter Plot of Years of Experience vs Age**\n",
    "\n",
    "\n",
    "    ![Scatter Plot of Years of Experience vs Age](./plots/scatter_Years_of_Experience_vs_Age.png)\n",
    "\n",
    "    - This plot displays the relationship between Years of Experience and Age. shows the expected distribution, sugesting the dataset is not particualarly skewed or missrepresented.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Box Plots**\n",
    "\n",
    "Box plots provide a summary of the distribution of numerical variables, highlighting the median, quartiles, and potential outliers:\n",
    "\n",
    "\n",
    "\n",
    "1. **Box Plot of Age**\n",
    "\n",
    "    ![Box Plot of Age](./plots/boxplot_Age.png)\n",
    "    - This plot shows the distribution of the Age variable, including the median, quartiles, and any outliers.\n",
    "\n",
    "2. **Box Plot of Years of Experience**\n",
    "\n",
    "    ![Box Plot of Years of Experience](./plots/boxplot_Years_of_Experience.png)\n",
    "    - This plot illustrates the distribution of the Years of Experience variable, highlighting the central tendency and spread.\n",
    "\n",
    "3. **Box Plot of Salary**\n",
    "\n",
    "    ![Box Plot of Salary](./plots/boxplot_Salary.png)\n",
    "    - This plot presents the distribution of the Salary variable, showing the median, quartiles, and outliers.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Histograms**\n",
    "\n",
    "Histograms provide a visual representation of the distribution of numerical variables:\n",
    "\n",
    "1. **Histogram of Age**\n",
    "\n",
    "    ![Histogram of Age](./plots/histogram_Age.png)\n",
    "    - This histogram shows the frequency distribution of the Age variable,  its distribution follows the expected normal curve for age just truncated due to the nature of work itself.\n",
    "\n",
    "2. **Histogram of Years of Experience**\n",
    "\n",
    "    ![Histogram of Years of Experience](./plots/histogram_Years_of_Experience.png)\n",
    "    - This histogram illustrates the frequency distribution of the Years of Experience variable, it shows a clear linear pattern with diminishing representation as experience increases wich is expected.\n",
    "\n",
    "3. **Histogram of Salary**\n",
    "\n",
    "    ![Histogram of Salary](./plots/histogram_Salary.png)\n",
    "    - This histogram presents the frequency distribution of the Salary variable, highlighting its distribution characteristics the distribution is not normal and sugest the dataset is biased towards the  140k range wich can make the model performace suffer, considering the size of the dataset pruning examples can hinder the hability of the model to converge, It is addressed with drop out layers for regularization on the NN model and the use of Huber loss to reduce the impact of outlier values.\n",
    "\n",
    "---\n",
    "#### **Count Plots**\n",
    "\n",
    "Count plots visualize the frequency of categorical variables:\n",
    "\n",
    "1. **Count Plot of Gender**\n",
    "\n",
    "    ![Count Plot of Gender](./plots/countplot_Gender.png)\n",
    "    - This plot shows the frequency distribution of the Gender variable, providing insights into the gender composition of the dataset.\n",
    "\n",
    "2. **Count Plot of Education Level**\n",
    "\n",
    "    ![Count Plot of Education Level](./plots/countplot_Education_Level.png)\n",
    "    - This plot illustrates the frequency distribution of the Education Level variable, revealing the educational background of the dataset.\n",
    "\n",
    "\n",
    "---\n",
    "#### **Top 10 Job Titles**\n",
    "\n",
    "The following plot visualizes the top 10 job titles in the dataset:\n",
    "\n",
    "1. **Top 10 Job Titles**\n",
    "\n",
    "    ![Top 10 Job Titles](./plots/top_10_job_titles.png)\n",
    "    - This plot shows the frequency of the top 10 job titles, highlighting the most common job titles in the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    "# **2 Feature engineering and data splitting**\n",
    "\n",
    "This step consist in the processing of the dataset for use in the process of training, here im also creating the objects needed for inference over new data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"21\"></a> \n",
    "## **2.1 Splitting data**\n",
    "\n",
    "Dividing the dataset into training and testing subsets to evaluate model performance. This step ensures an unbiased assessment of the model's capabilities.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#split the dataset into an 80 / 20 ratio for training and testing.\n",
    "\n",
    "X_train, X_test, y_train, y_test = feature_engenieering.split_data(cleansed_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"22\"></a> \n",
    "## **2.2 Data Normalization and Scaling**\n",
    "\n",
    " Normalizing and scaling data to standardize features, which helps improve model convergence and performance. unsing  Min-Max Scaling for the RNR model and Standardization for the NN model, the values for categorical features are treated case by case,for education its modeled as a linear relationship meaning 0 for bachellors m 1 for Masters and 2 for PHD, the Job title values since they have a strong correlation with salary but lack cardinality, the target encoder method seems to be the optimal. once the datasets are created are saved along with scalers and the job title target encoded table and their values in pkl format to be used later for training and inference respectibly.\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#normalize and scale the datasets using MinMaxScaler and target encoder for random forest\n",
    "\n",
    "normalized_X_train, te, scaler = feature_engenieering.normalize_train_data(X_train, y_train,MinMaxScaler())\n",
    "\n",
    "normalized_X_test = feature_engenieering.normalize_test_data(X_test, te, scaler)\n",
    "\n",
    "\n",
    "#normalize and scale the datasets using StandardScaler and target encoder for Neural Networks\n",
    "\n",
    "normalized_X_train_nn, te_nn, scaler_nn = feature_engenieering.normalize_train_data(X_train, y_train,RobustScaler(),\"nn_\")\n",
    "\n",
    "normalized_X_test_nn = feature_engenieering.normalize_test_data(X_test, te_nn, scaler_nn)\n",
    "\n",
    "feature_engenieering.save_datasets(X_train, X_test, y_train, y_test, normalized_X_train, normalized_X_train_nn, normalized_X_test, normalized_X_test_nn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"23\"></a> \n",
    "## **2.3 Feature Relationship Visualization**\n",
    "\n",
    "This step involves the creation of visual plots to asses the distribution and relationships of the selected features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_visualization.plot_feature_relationships(x_train=normalized_X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Job title encoded vs Salary**\n",
    "\n",
    "    ![Job title encoded vs Salary](./plots/job_title_encoded_vs_salary.png)\n",
    "    \n",
    "    - This pairplot visualizes the relationships between job title encoded to salary, specially important to gauge the feature modeling choice.\n",
    "\n",
    "2. **Pairplot of Training Data**\n",
    "\n",
    "    ![Pairplot](./plots/pairplot.png)\n",
    "    \n",
    "    - This pairplot visualizes the relationships between different pairs of features in the training data. It helps to understand the feature interactions and distributions.\n",
    "\n",
    "3. **Correlation Heatmap**\n",
    "\n",
    "    ![Correlation Heatmap](./plots/correlation_heatmap.png)\n",
    "    \n",
    "    - This heatmap shows the correlation between different features in the training data. It helps to identify highly correlated features and potential multicollinearity issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    "# **3 Model Training and Evaluation**\n",
    "\n",
    "The section includes the training and further feature and performance analisys over a Random Forest Regressor and a Neural Network model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"31\"></a> \n",
    "## **3.1 Baseline Model: Dummy Regressor**\n",
    "\n",
    "Establishing a baseline performance using a dummy regressor. This simple model provides a reference point for comparing advanced models.\n",
    "\n",
    "With the datasets created we use the train with MinMaxScaling dataset splits in a script to create a Random Forest Regressor using the scikit-learn framework.\n",
    "\n",
    "I started by traing a Dummy Reggressor to use as a baseline for model performance comparison and then train a Random Forest Regressor algorithm with hyperparameter tuning. We also evaluate the trained model by calculating metrics such as mean absolute error (MAE), root mean squared error (RMSE) and R-squared e (R2) and plot a scatterplot of predicted vs actual salaries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load generated datasets as pkl\n",
    "\n",
    "X_train, X_test, y_train, y_test, normalized_X_train, normalized_X_train_nn, normalized_X_test, normalized_X_test_nn = feature_engenieering.load_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy = modeling.train_dummy_regressor(normalized_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"32\"></a> \n",
    "## **3.2 Random Forest Regressor**\n",
    "\n",
    "Training a Random Forest Regressor, a robust ensemble model, to predict target variables. This model aggregates results from multiple decision trees. this step also includes Optimizing model performance by tuning hyperparameters and selecting the most promising model using a grid search technique with GridSearchCV \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a model using a random forest regressor algorithm and print out the predictions for the normalized test data.\n",
    "\n",
    "rf_model = modeling.train_model(normalized_X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Training and Evaluation Plots**\n",
    "\n",
    "The following plots provide insights into the performance and behavior of the trained models during the training and evaluation phases:\n",
    "\n",
    "---\n",
    "\n",
    "1. **Grid Search Heatmap**\n",
    "\n",
    "    ![Grid Search Heatmap](./plots/grid_search_heatmap.png)\n",
    "    \n",
    "    - This heatmap visualizes the performance of different hyperparameter combinations during the grid search process. It helps to identify the optimal hyperparameters that yield the best model performance.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Grid Search Line Plot**\n",
    "\n",
    "    ![Grid Search Line Plot](./plots/grid_search_lineplot.png)\n",
    "\n",
    "    - This line plot shows the performance metrics (e.g., accuracy, loss) for different hyperparameter combinations over the grid search iterations. It helps to understand the trend and stability of the model performance across different hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "These plots collectively provide a comprehensive view of the model's performance during the hyperparameter tuning process, highlighting the best-performing configurations and areas for potential improvement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"33\"></a> \n",
    "## **3.3 RFR initial testing**\n",
    "\n",
    "To quickly asses the training process validity a first fast evaluation is done followed by a boostraped test run to propperly asses confidence intervals on predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model from a pickle file\n",
    "rf_model = joblib.load(open('./models/random_forest_model.pkl', 'rb'))\n",
    "\n",
    "#use the test dataset to predict salaries based on the trained model for a first fast evaluation.\n",
    "eval = evaluation.evaluate_model(normalized_X_test, y_test, normalized_X_train,y_train, rf_model)\n",
    "\n",
    "#calculate metrics for the model\n",
    "eval = evaluation.calculate_metrics(normalized_X_test, y_test, rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RFR Model Evaluation Results**\n",
    "\n",
    "The following plots provide insights into the performance and behavior of the trained Random Forest Regressor model during the evaluation phase:\n",
    "\n",
    "---\n",
    "\n",
    "1. **Distribution of Residuals**\n",
    "\n",
    "    ![Distribution of Residuals](./plots/distribution_of_residuals.png)\n",
    "    \n",
    "    - This plot shows the distribution of residuals (the difference between actual and predicted values). It helps to identify any patterns or biases in the model's predictions.\n",
    "\n",
    "2. **Residuals vs. Predicted Salaries**\n",
    "\n",
    "    ![Residuals vs. Predicted Salaries](./plots/residuals_vs_predicted_salaries.png)\n",
    "    \n",
    "    - This scatter plot illustrates the relationship between residuals and predicted salaries. It helps to assess the model's accuracy and identify any systematic errors.\n",
    "\n",
    "3. **SHAP Feature Analisys**\n",
    "\n",
    "    ![Shap Feature Analisys](./plots/shap_summary_plot_rf.png)\n",
    "\n",
    "    - Bar chart showing roughly how much each feature contributes to the models output.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Model Performance with Confidence Intervals:**\n",
    "\n",
    "Mean Squared Error (MSE): 600793386.21 (95% CI: [290532348.25, 1021638682.27])\n",
    "\n",
    "Mean Absolute Error (MAE): 15023.79 (95% CI: [11485.43, 19054.41])\n",
    "\n",
    "R-squared Score (R²): 0.76 (95% CI: [0.62, 0.87])\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "<a id=\"34\"></a> \n",
    "## **3.4 Feature Selection Process**\n",
    "\n",
    "In the feature selection process, I initially used a Random Forest Regressor to capture non-linear relationships with relatively low data quantity for training. This approach provides fast training and explainability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Initial Model Performance with All Features**\n",
    "\n",
    "The initial model was trained with all features to gather information on relationships and model error (R-squared Score, R²). The heteroscedasticity observed in the middle of the distribution is likely due to the dataset's lack of examples to fill the distribution appropriately. However, the residuals distribution is acceptable, with no statistically significant outliers.\n",
    "\n",
    "**Performance Metrics:**\n",
    "- Mean Squared Error (MSE): 866,064,920.96\n",
    "- Mean Absolute Error (MAE): 17,982.33\n",
    "- R-squared Score (R²): 0.65\n",
    "\n",
    "**Residuals Distribution:**\n",
    "\n",
    "![img](./plots/residuals_distribution.png)\n",
    "\n",
    "**SHAP Values:**\n",
    "\n",
    "![Feature Importance](./plots/feature_importance.png)\n",
    "\n",
    "**Feature Correlation Heatmap:**\n",
    "\n",
    "![feature correlation heatmap](./plots/feature_correlation_heatmap_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Dropping Gender**\n",
    "\n",
    "Gender was dropped as it showed a low correlation with salary and low feature importance. This improved the model's performance.\n",
    "\n",
    "**Performance Metrics:**\n",
    "- Mean Squared Error (MSE): 600,148,032.12\n",
    "- Mean Absolute Error (MAE): 14,748.68\n",
    "- R-squared Score (R²): 0.76\n",
    "\n",
    "**Residuals Distribution:**\n",
    "\n",
    "![img](./plots/residuals_distribution_2.png)\n",
    "\n",
    "**SHAP Values:**\n",
    "\n",
    "![Feature Importance](./plots/feature_importance_2.png)\n",
    "\n",
    "**Feature Correlation Heatmap:**\n",
    "\n",
    "![feature correlation heatmap](./plots/feature_correlation_heatmap_2.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Dropping Education Level**\n",
    "\n",
    "Surprisingly, dropping the education level, which had a low correlation with salary, resulted in worse model performance.\n",
    "\n",
    "**Performance Metrics:**\n",
    "- Mean Squared Error (MSE): 808,374,516.33\n",
    "- Mean Absolute Error (MAE): 17,240.95\n",
    "- R-squared Score (R²): 0.67\n",
    "\n",
    "**Residuals Distribution:**\n",
    "\n",
    "![img](./plots/residuals_distribution_3.png)\n",
    "\n",
    "**SHAP Values:**\n",
    "\n",
    "![Feature Importance](./plots/feature_importance_3.png)\n",
    "\n",
    "**Feature Correlation Heatmap:**\n",
    "\n",
    "![feature correlation heatmap](./plots/feature_correlation_heatmap_3.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "The main feature to drop is gender due to its high noise-to-signal ratio and low correlation with the target variable. The education level, although loosely correlated, still provides an advantage to the model, indicating a relevant relationship with the target.\n",
    "\n",
    "After the initial transformations and feature selection, the final model includes the following features in order of relevance based on SHAP scores and correlation maps:\n",
    "- Job Title\n",
    "- Age\n",
    "- Years of Experience\n",
    "- Education Level\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.5\"></a> \n",
    "\n",
    "## **3.5 Neural Network Model**\n",
    "\n",
    "In this step I create a second model to test performance of different approaches in this specific problem. I'm using the same dataset and features as the random forest regressor but normalized using the stardad scaling wich provides better performances on NN sequiential models, the hypotesis is that this approach can generalize better the dataset with sufficient Paramenters\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3.5.1 Model Overview**\n",
    "The neural network is structured as a feed-forward sequential model with a funnel architecture, designed for regression tasks. The model processes standardized features through multiple dense layers with decreasing neuron counts.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.5.2 Architecture Diagram**\n",
    "\n",
    "```\n",
    "Input Layer (shape: features) →\n",
    "Dense(64, ReLU) → Dropout(0.2) →\n",
    "Dense(32, ReLU) → Dropout(0.2) →\n",
    "Dense(16, ReLU) →\n",
    "Dense(1, linear)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3.5.3 Design Choices**\n",
    "\n",
    "1. **Input Processing**\n",
    "   - Uses RobustScaler normalized features (in testing it gave a slight advantage on R squared metrics)\n",
    "   - transforms the target to a log representation\n",
    "   - Adapts to input shape automatically\n",
    "\n",
    "2. **Hidden Layers**\n",
    "   - Funnel architecture: 64 → 32 → 16 neurons\n",
    "   - ReLU activation for non-linearity\n",
    "   - Dropout layers (0.2) for regularization\n",
    "\n",
    "3. **Training Configuration**\n",
    "   - Adam optimizer (lr=0.001)\n",
    "   - Mean Squared Error loss\n",
    "   - Early stopping (patience=10)\n",
    "   - Validation split: 10%\n",
    "   - Batch size: 32\n",
    "   - Max epochs: 1000\n",
    "\n",
    "---\n",
    "\n",
    "### **3.5.4 Comparison with Random Forest**\n",
    "\n",
    "| Aspect | Neural Network | Random Forest |\n",
    "|--------|---------------|---------------|\n",
    "| Scaling | Requires normalization | Scale-invariant |\n",
    "| Training | Iterative, gradient-based | Ensemble, parallel |\n",
    "| Hyperparameters | Layer sizes, learning rate | Trees, depth, features |\n",
    "| Interpretability | Less interpretable | More interpretable |\n",
    "\n",
    "---\n",
    "\n",
    "### **3.5.5 Hypothesis**\n",
    "The neural network approach may capture complex non-linear relationships in the salary data that tree-based methods might miss, potentially leading to better generalization on unseen data.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the Neural Network model\n",
    "modeling.train_NN_model(normalized_X_train_nn,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./plots/nn_training_loss.png)\n",
    "\n",
    "The plot represents the loss over time during model training, it can be seen that the model converged at around 175 epochs triggering Early stop to prevent overfit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"36\"></a> \n",
    "## **3.6 Neural Network Model initial testing**\n",
    "\n",
    "To quickly asses the training process validity a first fast evaluation is done followed by a boostraped test run to propperly asses confidence intervals on predictions. folowing the same methology as on the RFR model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model from a keras file\n",
    "nn_model = keras.models.load_model('./models/neural_network_model.keras')\n",
    "\n",
    "#use the test dataset to predict salaries based on the trained model for a first fast evaluation.\n",
    "eval = evaluation.evaluate_NN_model(normalized_X_test_nn, y_test,normalized_X_train_nn, nn_model)\n",
    "\n",
    "#calculate metrics for the model\n",
    "metrics = evaluation.calculate_metrics(normalized_X_test_nn, y_test, nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NN Model Evaluation Results**\n",
    "\n",
    "The following plots provide insights into the performance and behavior of the trained Neural Network model during the evaluation phase:\n",
    "\n",
    "---\n",
    "\n",
    "1. **Distribution of Residuals**\n",
    "\n",
    "    ![Distribution of Residuals](./plots/distribution_of_residuals_nn.png)\n",
    "    \n",
    "    - This plot shows the distribution of residuals (the difference between actual and predicted values). the plot shows a normal distribution for the residuals aroud 0 wich means the model is behavioring as expected.\n",
    "\n",
    "2. **Residuals vs. Predicted Salaries**\n",
    "\n",
    "    ![Residuals vs. Predicted Salaries](./plots/residuals_vs_predicted_salaries_nn.png)\n",
    "    \n",
    "    - This scatter plot illustrates the relationship between residuals and predicted salaries. It shows a beter heterodecendacity with respect of the RFR model, consistent with the expected behavior.\n",
    "\n",
    "\n",
    "3. **SHAP Feature Analisys**\n",
    "\n",
    "    ![Shap Feature Analisys](./plots/shap_summary_plot_nn.png)\n",
    "\n",
    "    - Bar chart showing roughly how much each feature contributes to the models output.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Model Performance with Confidence Intervals:**\n",
    "\n",
    "Mean Squared Error (MSE): 491423944.87 (95% CI: [260609071.86, 850637893.20])\n",
    "\n",
    "Mean Absolute Error (MAE): 15287.96 (95% CI: [12594.30, 18564.54])\n",
    "\n",
    "R-squared Score (R²): 0.80 (95% CI: [0.68, 0.89])\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    "# **4 Model Comparison**\n",
    "\n",
    "Comparing the performance of all models using metrics such as accuracy, mean squared error (MSE), and R-squared calculated as confidence intervals to determine the best-performing model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three models: Dummy Regressor, Random Forest Regressor, and Neural Network\n",
    "dummy_model = joblib.load(open('./models/dummy_reggresor_model.pkl', 'rb'))\n",
    "rf_model = joblib.load(open('./models/random_forest_model.pkl', 'rb'))\n",
    "nn_model = keras.models.load_model('./models/neural_network_model.keras')\n",
    "\n",
    "# Prepare the models data dictionary\n",
    "models_data = {\n",
    "    'Dummy Regressor': (dummy_model, normalized_X_test, y_test),\n",
    "    'Random Forest Regressor': (rf_model, normalized_X_test, y_test),\n",
    "    'Neural Network': (nn_model, normalized_X_test_nn, y_test)\n",
    "}\n",
    "\n",
    "# Call the model comparison module\n",
    "metrics = model_compare.compare_models(models_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"41\"></a> \n",
    "\n",
    "### **4.1 Final Model Selection**\n",
    "\n",
    "\n",
    "After generation both aproaches for modeling the problem I settled on using the neural network approach since it seems to capture better the relationships between the features and is in general more precise.\n",
    "\n",
    "---\n",
    "\n",
    "we can observe the comparisons on the following charts:\n",
    "\n",
    "![img](./plots/predicted_vs_actual_values_model_comparison.png)\n",
    "\n",
    "\n",
    "![img](./plots/error_model_comparison.png)\n",
    "\n",
    "\n",
    "![img](./plots/residuals_distribution_model_comparison.png)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Model Performance Summary with Confidence Intervals**:\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "#### **Random Forest**:\n",
    "\n",
    "MSE: 591312507.301 (95% CI: [266726953.202, 1092637456.989])\n",
    "\n",
    "MAE: 14592.480 (95% CI: [10587.479, 19339.949])\n",
    "\n",
    "R2: 0.761 (95% CI: [0.635, 0.872])\n",
    "\n",
    "---\n",
    "\n",
    "#### **Neural Network**:\n",
    "\n",
    "MSE: 410098860.336 (95% CI: [172791209.008, 806977256.100])\n",
    "\n",
    "MAE: 12925.254 (95% CI: [9796.850, 17131.055])\n",
    "\n",
    "R2: 0.831 (95% CI: [0.724, 0.912])\n",
    "\n",
    "---\n",
    "\n",
    "#### **Dummy Regresson**:\n",
    "\n",
    "MSE: 2461198619.228 (95% CI: [1834308694.379, 3231299942.415])\n",
    "\n",
    "MAE: 41588.630 (95% CI: [35333.317, 48193.295])\n",
    "\n",
    "R2: -0.015 (95% CI: [-0.080, -0.000])\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    "\n",
    "# **5 Inference**\n",
    "\n",
    "The inference process involves using the trained neural network model to make predictions on new or unseen data. This section outlines the steps to load the model, preprocess the input data, and generate predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.1 Loading the Model**\n",
    "\n",
    "First, we load the trained neural network model from the saved file. This model has been trained and validated on the dataset, and it is now ready to be used for inference.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.2 Preparing the Input Data**\n",
    "\n",
    "The input data must be preprocessed in the same way as the training data. This includes encoding categorical variables and scaling numerical features. The preprocessing steps ensure that the input data is in the correct format for the model to make accurate predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.3 Making Predictions**\n",
    "\n",
    "Once the model is loaded and the input data is preprocessed, we can use the model to make predictions. The predictions will be the estimated salaries based on the input features.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.4 Interactive Input Form**\n",
    "\n",
    "To facilitate the inference process, I have created an interactive input form using ipywidgets. This form allows users to input the necessary features and obtain salary predictions in real-time.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code cell is self-contained and can be run independently from the rest of the notebook. It will load the trained models and the test dataset, and then it will display a form to input the job title, company name, and job description. After filling the form, the user can click the \"Predict Salary\" button to get the salary prediction for the input data.\n",
    "\n",
    "from src import inference\n",
    "from src.inference_jupyter_form import create_input_form\n",
    "\n",
    "job_titles = inference.get_unique_job_titles(prefix=\"nn_\")\n",
    "form = create_input_form(job_titles)\n",
    "display(form)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
